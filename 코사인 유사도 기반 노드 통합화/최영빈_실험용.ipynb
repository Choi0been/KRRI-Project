import pickle

# entity_idx.pkl 파일 로드
with open("entity_idx.pkl", "rb") as f:
    entity_idx = pickle.load(f)

# 노드 개수 출력
print(f"Number of nodes before merging: {len(entity_idx)}")
# =========================================
from collections import defaultdict
from sentence_transformers import SentenceTransformer, util
import pickle

def merge_similar_texts(simC, texts):
    model = SentenceTransformer("jhgan/ko-sroberta-multitask")  # 비지도 학습용 모델 선택
    
    # 텍스트 벡터 인코딩
    text_vec = model.encode(texts)
    
    # 코사인 유사도 계산
    cos_sim = util.pytorch_cos_sim(text_vec, text_vec)
    
    similar_texts = defaultdict(list)
    similar_texts_scores = defaultdict(list)  # 유사도 저장용 딕셔너리

    for i in range(len(texts)):
        for j in range(i+1, len(texts)):
            if cos_sim[i, j] >= simC:
                similar_texts[texts[i]].append(texts[j])
                similar_texts[texts[j]].append(texts[i])
                
                similar_texts_scores[(texts[i], texts[j])] = cos_sim[i, j].item()  # 유사도 저장
    
    merged_texts = {}

    for text, similar in similar_texts.items():
        merged_texts[text] = text  # 초기에는 자기 자신을 대표로 설정
        
        for s in similar:
            if s in merged_texts:
                merged_texts[s] = text  # 대표로 설정된 텍스트로 통합
    
    return merged_texts, similar_texts_scores  # 유사도 딕셔너리 반환

# 엔티티 리스트 로드
with open("entity_idx.pkl", "rb") as f:
    entity_idx = pickle.load(f)

entity_list = list(entity_idx.keys())

# 훈련 데이터 준비
training_data = []

# 엔티티 리스트의 엔티티를 모두 추가
training_data.extend(entity_list)

# 텍스트 통합
merged_texts, similar_texts_scores = merge_similar_texts(0.75, training_data)

# 결과 출력
print("Original Texts:")
for text in training_data:
    print(text)

print("\nMerged Texts:")
for original, merged in merged_texts.items():
    print(f"{original} -> {merged}")

num_merged_entities = len(set(merged_texts.values()))
print(f"\nNumber of Merged Entities: {num_merged_entities}")

# 유사도 출력
print("\nSimilarity Scores:")
for texts, score in similar_texts_scores.items():
    print(f"{texts[0]} and {texts[1]}: {score}")
    
# 피클 파일로 저장
with open('merged_texts.pkl', 'wb') as f:
    pickle.dump(merged_texts, f)
print("Merged Texts saved to merged_texts.pkl")    
import pickle
from collections import defaultdict
from sentence_transformers import SentenceTransformer, util

# =========================================================================

# 유사한 텍스트를 그룹화하는 함수
def merge_similar_texts(simC, texts):
    model = SentenceTransformer("jhgan/ko-sroberta-multitask")  # 비지도 학습용 모델 선택
    
    # 텍스트 벡터 인코딩
    text_vec = model.encode(texts)
    
    # 코사인 유사도 계산
    cos_sim = util.pytorch_cos_sim(text_vec, text_vec)
    
    similar_texts = defaultdict(list)
    similar_texts_scores = defaultdict(list)  # 유사도 저장용 딕셔너리

    # 유사 텍스트 그룹화 (병합 전)
    for i in range(len(texts)):
        for j in range(i+1, len(texts)):
            if cos_sim[i, j] >= simC:
                similar_texts[texts[i]].append(texts[j])
                similar_texts[texts[j]].append(texts[i])
                
                similar_texts_scores[(texts[i], texts[j])] = cos_sim[i, j].item()  # 유사도 저장
    
    # 코사인 유사도에 의해 묶인 그룹 출력 (병합 전)
    print("\n--- Grouped Texts Before Merging ---")
    for text, similar in similar_texts.items():
        print(f"Text: {text}")
        print(f"Similar Texts (grouped by cosine similarity): {similar}")
        print("------")

    merged_texts = {}

    # 병합 과정
    for text, similar in similar_texts.items():
        merged_texts[text] = text  # 초기에는 자기 자신을 대표로 설정
        
        for s in similar:
            if s in merged_texts:
                merged_texts[s] = text  # 대표로 설정된 텍스트로 통합
    
    return merged_texts, similar_texts_scores  # 유사도 딕셔너리 반환

# 엔티티 리스트 로드
with open("after_entity_idx.pkl", "rb") as f:
    entity_idx = pickle.load(f)

entity_list = list(entity_idx.keys())

# 훈련 데이터 준비
training_data = []

# 엔티티 리스트의 엔티티를 모두 추가
training_data.extend(entity_list)

# 텍스트 통합
merged_texts, similar_texts_scores = merge_similar_texts(0.75, training_data)

# ===========================================================

import pickle
from collections import defaultdict
from sentence_transformers import SentenceTransformer, util

# 유사한 텍스트를 그룹화하는 함수
def merge_similar_texts(simC, texts):
    model = SentenceTransformer("jhgan/ko-sroberta-multitask")  # 비지도 학습용 모델 선택
    
    # 텍스트 벡터 인코딩
    text_vec = model.encode(texts)
    
    # 코사인 유사도 계산
    cos_sim = util.pytorch_cos_sim(text_vec, text_vec)
    
    similar_texts = defaultdict(list)
    similar_texts_scores = defaultdict(list)  # 유사도 저장용 딕셔너리

    # 유사 텍스트 그룹화 (병합 전)
    for i in range(len(texts)):
        for j in range(i+1, len(texts)):
            if cos_sim[i, j] >= simC:
                similar_texts[texts[i]].append(texts[j])
                similar_texts[texts[j]].append(texts[i])
                
                similar_texts_scores[(texts[i], texts[j])] = cos_sim[i, j].item()  # 유사도 저장
    
    # 코사인 유사도에 의해 묶인 그룹 출력 (병합 전)
    print("\n--- Grouped Texts Before Merging ---")
    for text, similar in similar_texts.items():
        print(f"Text: {text}")
        print(f"Similar Texts (grouped by cosine similarity): {similar}")
        print("------")

    merged_texts = {}

    # 병합 과정
    for text, similar in similar_texts.items():
        merged_texts[text] = text  # 초기에는 자기 자신을 대표로 설정
        
        for s in similar:
            if s in merged_texts:
                merged_texts[s] = text  # 대표로 설정된 텍스트로 통합
    
    return merged_texts, similar_texts_scores  # 유사도 딕셔너리 반환

# 엔티티 리스트 로드
with open("before_entity_idx.pkl", "rb") as f:
    entity_idx = pickle.load(f)

entity_list = list(entity_idx.keys())

# 훈련 데이터 준비
training_data = []

# 엔티티 리스트의 엔티티를 모두 추가
training_data.extend(entity_list)

# 텍스트 통합
merged_texts_fixed, similar_texts_scores = merge_similar_texts(0.75, training_data)


# ===================================================================================

import pickle
from collections import defaultdict
from sentence_transformers import SentenceTransformer, util
import numpy as np

def calculate_density(simC, texts):
    model = SentenceTransformer("jhgan/ko-sroberta-multitask")
    text_vec = model.encode(texts)  # 텍스트 벡터화
    cos_sim = util.pytorch_cos_sim(text_vec, text_vec)  # 코사인 유사도 행렬 계산

    density = {}  # 각 키워드의 밀도 저장
    for i, text in enumerate(texts):
        # 0.75 이상의 유사도를 가진 키워드 개수를 계산
        count = (cos_sim[i] >= simC).sum().item() - 1  # 자기 자신 제외
        density[text] = count
    
    return density

# 엔티티 리스트 로드
with open("before_entity_idx.pkl", "rb") as f:
    entity_idx = pickle.load(f)

entity_list = list(entity_idx.keys())

# 밀도 계산
density_scores = calculate_density(0.75, entity_list)

# 결과 출력
for key, value in density_scores.items():
    print(f"{key}:  {value}")

# =========================================================================

# 밀도의 최댓값과 최솟값 계산
max_density = max(density_scores.values())
min_density = min(density_scores.values())

# 최댓값과 최솟값을 가지는 키워드 찾기
max_density_keywords = [k for k, v in density_scores.items() if v == max_density]
min_density_keywords = [k for k, v in density_scores.items() if v == min_density]

# 결과 출력
print("\n--- Density Statistics ---")
print(f"Max Density: {max_density}, Keywords: {max_density_keywords}")
print(f"Min Density: {min_density}, Keywords: {min_density_keywords}")

# ==============================================================================

# # 동적 임계값 계산
# thresholds = {
#     k: 0.70 + 0.1 * ((v - min_density) / (max_density - min_density))
#     for k, v in density_scores.items()
# }


#   0306  평균 연결 개수 : 2.71%,  노드 수 : -1.37%,  엣지 수 :1.30% * 클러스터 밀도 균형 개선율 :    4.27%
def compute_dynamic_thresholds(density_scores):
    density_values = np.array(list(density_scores.values()))
    Q1 = np.percentile(density_values, 25)
    Q2 = np.percentile(density_values, 50)  # 중위수 (Median)
    Q3 = np.percentile(density_values, 75)

    temp_thresholds = {}
    for text, density in density_scores.items():
        if density <= Q2:
            temp_threshold = 0.70 + 0.05 * (Q2 - density) / (Q2 - Q1)
        else:
            temp_threshold = 0.75 + 0.05 * (density - Q2) / (Q3 - Q1)
        temp_thresholds[text] = min(0.80, max(0.70, temp_threshold)) # 0.70~0.80 범위 제한

    return temp_thresholds  # 함수 내부에서는 다른 변수명 사용


# # 0307 4분위 수 방식식
# #* 네트워크 증가율(%)➡️  평균 연결 개수 : 0.89%,  노드 수 : 1.59%,  엣지 수 :2.50% * 클러스터 밀도 균형 개선율 :    4.28%
# def compute_dynamic_thresholds(density_scores):
#     density_values = np.array(list(density_scores.values()))
#     Q1 = np.percentile(density_values, 25)
#     Q2 = np.percentile(density_values, 50)  # 중위수 (Median)
#     Q3 = np.percentile(density_values, 75)
    
#     temp_thresholds = {}
#     for text, density in density_scores.items():
#         if density <= Q1:
#             # Q1 이하일 경우 감소량을 완화하고 최저 0.72로 제한
#             temp_threshold = 0.72 + 0.02 * (Q2 - density) / (Q2 - Q1)
#         elif density <= Q2:
#             temp_threshold = 0.74 + 0.04 * (Q2 - density) / (Q2 - Q1)
#         elif density <= Q3:
#             temp_threshold = 0.75 + 0.04 * (density - Q2) / (Q3 - Q1)
#         else:
#             # Q3 이상일 경우 증가폭 제한 (최대 0.78)
#             temp_threshold = 0.76 + 0.02 * (density - Q3) / (Q3 - Q1)

#         # 최종 범위 제한
#         temp_thresholds[text] = min(0.78, max(0.72, temp_threshold))

#     return temp_thresholds  # 함수 내부에서는 다른 변수명 사용



thresholds = compute_dynamic_thresholds(density_scores)

# 결과 출력
print("\n--- Dynamic Thresholds ---")
for key, value in thresholds.items():
    print(f"{key}: {value:.4f}")

# =======================================================================

import pickle
from collections import defaultdict
from sentence_transformers import SentenceTransformer, util
import numpy as np

def merge_similar_texts_dynamic(texts, thresholds):
    model = SentenceTransformer("jhgan/ko-sroberta-multitask")
    text_vec = model.encode(texts)  # 텍스트 벡터화
    cos_sim = util.pytorch_cos_sim(text_vec, text_vec)  # 코사인 유사도 행렬

    similar_texts = defaultdict(list)
    similar_texts_scores = defaultdict(list)

    # 유사 텍스트 그룹화 (병합 전)
    for i in range(len(texts)):
        for j in range(i + 1, len(texts)):
            threshold_i = thresholds[texts[i]]
            threshold_j = thresholds[texts[j]]
            dynamic_threshold = min(threshold_i, threshold_j)  # 두 개 중 작은 임계값 적용

            if cos_sim[i, j] >= dynamic_threshold:
                similar_texts[texts[i]].append(texts[j])
                similar_texts[texts[j]].append(texts[i])
                similar_texts_scores[(texts[i], texts[j])] = cos_sim[i, j].item()  # 유사도 저장

    merged_texts = {}

    # 병합 과정
    for text, similar in similar_texts.items():
        merged_texts[text] = text  # 초기에는 자기 자신을 대표로 설정
        
        for s in similar:
            if s in merged_texts:
                merged_texts[s] = text  # 대표로 설정된 텍스트로 통합

    return merged_texts, similar_texts_scores

# 엔티티 리스트 로드
with open("before_entity_idx.pkl", "rb") as f:
    entity_idx = pickle.load(f)

entity_list = list(entity_idx.keys())

# 밀도 계산
density_scores = calculate_density(0.75, entity_list)

# 최댓값, 최솟값 계산
min_density = min(density_scores.values())
max_density = max(density_scores.values())


# # 결과 저장
# thresholds = compute_dynamic_thresholds(density_scores)

# 동적 임계값 기반 텍스트 병합 실행
merged_texts_dynamic, similar_texts_scores = merge_similar_texts_dynamic(entity_list, thresholds)

# 결과 출력
print("\n--- Merged Texts ---")
for key, value in merged_texts.items():
    print(f"{key} → {value}")

# ================================================================

import networkx as nx

def compute_network_complexity(merged_texts, similar_texts_scores):
    G = nx.Graph()
    
    for text1, text2 in similar_texts_scores.keys():
        if text1 in merged_texts and text2 in merged_texts and merged_texts[text1] == merged_texts[text2]:
            G.add_edge(text1, text2)

    num_nodes = len(G.nodes())
    num_edges = len(G.edges())
    avg_degree = sum(dict(G.degree()).values()) / num_nodes if num_nodes > 0 else 0  # ZeroDivisionError 방지

    return avg_degree, num_nodes, num_edges

# 기존 연구(0.75) 네트워크 복잡도
complexity_fixed = compute_network_complexity(merged_texts_fixed, similar_texts_scores)

# 동적 임계값 네트워크 복잡도
complexity_dynamic = compute_network_complexity(merged_texts_dynamic, similar_texts_scores)

print(f"* 기존 연구(고정 0.75)  평균 연결 개수 : {complexity_fixed[0]:.2f},     노드 수: {complexity_fixed[1]},     엣지 수: {complexity_fixed[2]}")
print(f"* 동적 임계값 적용 후   평균 연결 개수 : {complexity_dynamic[0]:.2f},     노드 수: {complexity_dynamic[1]},     엣지 수: {complexity_dynamic[2]}")
#print(f"➡️ 네트워크 복잡도 감소율: {(complexity_dynamic[0] - complexity_fixed[0]) / complexity_fixed[0] * 100:.2f}%")
print(f"* 네트워크 증가율(%)➡️  평균 연결 개수 : {(complexity_dynamic[0] - complexity_fixed[0]) / complexity_fixed[0] * 100:.2f}%,  노드 수 : {(complexity_dynamic[1] - complexity_fixed[1]) / complexity_fixed[1] * 100:.2f}%,  엣지 수 :{(complexity_dynamic[2] - complexity_fixed[2]) / complexity_fixed[2] * 100:.2f}%")

# ================================================================

def compute_cluster_size_std(merged_texts):
    cluster_sizes = defaultdict(int)
    for text, cluster in merged_texts.items():
        cluster_sizes[cluster] += 1
    
    return np.std(list(cluster_sizes.values()))

size_std_fixed = compute_cluster_size_std(merged_texts_fixed)
size_std_dynamic = compute_cluster_size_std(merged_texts_dynamic)


print(f"* 기존 연구(고정 0.75)  평균 연결 개수 : {complexity_fixed[0]:.2f},     노드 수: {complexity_fixed[1]},     엣지 수: {complexity_fixed[2]}")
print(f"* 동적 임계값 적용 후   평균 연결 개수 : {complexity_dynamic[0]:.2f},     노드 수: {complexity_dynamic[1]},     엣지 수: {complexity_dynamic[2]}")
print(f"* 네트워크 증가율(%)➡️  평균 연결 개수 : {(complexity_dynamic[0] - complexity_fixed[0]) / complexity_fixed[0] * 100:.2f}%,  노드 수 : {(complexity_dynamic[1] - complexity_fixed[1]) / complexity_fixed[1] * 100:.2f}%,  엣지 수 :{(complexity_dynamic[2] - complexity_fixed[2]) / complexity_fixed[2] * 100:.2f}%")

print(f"* 기존 연구(고정 0.75) 클러스터 크기 표준편차   :   {size_std_fixed:.2f}")
print(f"* 동적 임계값 적용 후 클러스터 크기 표준편차    :   {size_std_dynamic:.2f}")
print(f"* 클러스터 밀도 균형 개선율 :    {(size_std_fixed - size_std_dynamic) / size_std_fixed * 100:.2f}%")

# ================================================================

import matplotlib.pyplot as plt


plt.hist(thresholds, bins=10, alpha=0.72, color='blue', edgecolor='black')
plt.axvline(0.75, color='red', linestyle='dashed', linewidth=2, label="Fixed 0.75")
plt.xlabel("Dynamic Thresholds")
plt.ylabel("Frequency")
plt.legend()
plt.title("Distribution of Dynamic Thresholds")
plt.show()

# ================================================================
from transformers import AutoModel, AutoTokenizer
import torch
import torch.nn.functional as F

# 모델 및 토크나이저 로드
model_name = "jhgan/ko-sroberta-multitask"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

# 입력 문장
sentences = ["화재", "전소"]

# 토큰화 및 입력 변환
inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors="pt")

# 모델을 이용해 임베딩 추출
with torch.no_grad():
    outputs = model(**inputs)
    embeddings = outputs.last_hidden_state[:, 0, :]  # [CLS] 토큰의 임베딩 사용

# 코사인 유사도 계산
cos_sim = F.cosine_similarity(embeddings[0].unsqueeze(0), embeddings[1].unsqueeze(0))

print(f"화재와 전소의 코사인 유사도: {cos_sim.item():.4f}")
